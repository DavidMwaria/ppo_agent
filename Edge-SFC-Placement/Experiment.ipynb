{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "import math\n",
    "import random\n",
    "import copy\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "# import self-defined modules\n",
    "from EdgeTopo import *\n",
    "from Env import *\n",
    "from Baseline import *\n",
    "from Agent_DQN import *\n",
    "from Critic import Critic\n",
    "from Scheduler import *\n",
    "from Plot import *\n",
    "from Validation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using CUDA \n",
    "use_cuda = torch.cuda.is_available() \n",
    "device   = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_model(sch_lst,memory_warmup_size, learn_freq, batch_size):\n",
    "    update_num = 0\n",
    "    for k in sch_lst.sch_dict.keys():\n",
    "        if len(sch_lst.sch_dict[k].rpm) > memory_warmup_size and (sch_lst.sch_dict[k].sub_step % learn_freq == 0):\n",
    "            (batch_obs, batch_action, batch_reward,\n",
    "                 batch_next_obs, batch_done) = sch_lst.sch_dict[k].rpm.sample(batch_size)\n",
    "            sch_lst.sch_dict[k].agent.learn(batch_obs, batch_action, batch_reward, batch_next_obs, batch_done)\n",
    "            update_num+=1\n",
    "\n",
    "    return update_num  # num of agent trained\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def run_episode(env,env_HR,env_RH, sch_lst, memory_warmup_size, learn_freq, batch_size):\n",
    "    total_reward = 0\n",
    "    obs = env.reset()\n",
    "    sch_lst.step_reset()\n",
    "    pth_sch = sch_lst.sch_dict['path'] # get path scheduler\n",
    "    ptn_sch = sch_lst.sch_dict['22'] # get pattern scheduler\n",
    "    step = 0\n",
    "    while True:\n",
    "        step += 1\n",
    "        '''path selection'''\n",
    "        # sample path action from all possible actions\n",
    "        if not env.use_path_h :\n",
    "            if sch_lst.all_warmup:\n",
    "                path_action = pth_sch.agent.sample(obs)\n",
    "            else:\n",
    "                path_action = h_path(env) # make rpm accumulating faster\n",
    "            pth_sch.sub_step += 1\n",
    "        else:\n",
    "            path_action = h_path(env)\n",
    "        # print('path_action = ', path_action)\n",
    "        '''pattern selection'''\n",
    "        if not env.use_pattern_h and path_action!=0 :\n",
    "            ptn_sch = get_scheduler(env, path_action, sch_lst.sch_dict)\n",
    "            pattern_action = ptn_sch.agent.sample(obs)\n",
    "            if pattern_action != 0:\n",
    "                ptn_sch.config_ptn(pattern_action, env)\n",
    "            ptn_sch.sub_step += 1\n",
    "        elif path_action!=0 :\n",
    "            pattern_action =  h_pattern(env, path_action)\n",
    "        else: # ignore pattern selection if path selection failed\n",
    "            pattern_action = 0\n",
    "        # deployment step\n",
    "        next_obs, reward, done, _ = env.step(path_action)\n",
    "        # append rpm\n",
    "        if not env.use_path_h :\n",
    "            pth_sch.rpm.append((obs, path_action, reward, next_obs, done))\n",
    "        if not env.use_pattern_h :\n",
    "            ptn_sch.rpm.append((obs, pattern_action, reward, next_obs, done))\n",
    "\n",
    "        # train model if needed\n",
    "        train_model(sch_lst,memory_warmup_size, learn_freq, batch_size)\n",
    "        # update reward and obs\n",
    "        total_reward += reward\n",
    "        obs = next_obs\n",
    "\n",
    "        if done:\n",
    "            if not plot_explicit:\n",
    "                episode_rewards.append(total_reward)\n",
    "                plot_rewards()\n",
    "            break\n",
    "    #env.stat.push_result() # write statistic data to file\n",
    "    validate_episode(env,env_HR,env_RH, sch_lst, env.use_path_h, env.use_pattern_h) # validate using the same req list\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "def train(continue_train=False,\n",
    "          model_save_path='best_model', learn_freq=5, memory_size=20000,\n",
    "          memory_warmup_size=2000, batch_size=32, learning_rate=0.001,\n",
    "          gamma=0.4, alpha=0.9, max_episode=1000, ):\n",
    "\n",
    "    topo1 = Topo(4, 10)\n",
    "    env = Environment(topo1)\n",
    "    env_HR = copy.deepcopy(env)  # env to simulate H path + RL pattern\n",
    "    env_RH = copy.deepcopy(env)  # env to simulate RL path + H pattern\n",
    "    env.stat.start_recording()\n",
    "    # observation vector of network edges, nodes and current req\n",
    "    obs_dim = 2*topo1.l_num + topo1.c_num + env.req_encode_size\n",
    "    #print('obs_dim = ',obs_dim)\n",
    "    sch_lst = make_sch_list(memory_warmup_size,memory_size, obs_dim, obs_dim, learning_rate, gamma, alpha,topo1)\n",
    "    # pre-store some data in memory pool for warming up\n",
    "    turn=0\n",
    "    base_pth_h = True\n",
    "    base_ptn_h = True\n",
    "    env.base_reward = validate_episode(env,env_HR,env_RH, sch_lst,\\\n",
    "                                       base_pth_h, base_ptn_h)\n",
    "    # warm up each agent by adding enough samples in its rpm\n",
    "    if (not env.use_path_h) and (not env.use_pattern_h):\n",
    "        while not sch_lst.check_warmup():\n",
    "            print('turn = ', turn)\n",
    "            run_episode(env,env_HR,env_RH, sch_lst, memory_warmup_size,\n",
    "                        learn_freq, batch_size)\n",
    "            turn+=1\n",
    "\n",
    "    # start train\n",
    "    print('training ...')\n",
    "    episode = 0\n",
    "    while episode < max_episode:  # 训练max_episode个回合，test部分不计算入episode数量\n",
    "        # train part\n",
    "        for i in range(0, 100):\n",
    "            total_reward = run_episode(\n",
    "                env,env_HR,env_RH, sch_lst, memory_warmup_size, learn_freq,\\\n",
    "                batch_size)\n",
    "            episode += 1\n",
    "\n",
    "        print('total reward in episode ',episode,': ',total_reward)\n",
    "    env.stat.end_recording()\n",
    "    #agent.save(model_save_path)\n",
    "\n",
    "print('Complete')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(continue_train=False,\n",
    "          model_save_path='best_model', learn_freq=5, memory_size=20000,\n",
    "          memory_warmup_size=2000, batch_size=32, learning_rate=0.001,\n",
    "          gamma=0.4, alpha=0.9, max_episode=600, )\n",
    "if plot_explicit:\n",
    "    plot_rewards(show_result=True)\n",
    "    plt.ioff()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "b09ec625f77bf4fd762565a912b97636504ad6ec901eb2d0f4cf5a7de23e1ee5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
